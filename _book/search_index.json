[["index.html", "Where’s the Randomness Notes Adapted from Leo Breiman’s Applied Statistics Class of 1991 Proloque", " Where’s the Randomness Notes Adapted from Leo Breiman’s Applied Statistics Class of 1991 Stat Student 2024-07-20 Proloque | One problem in the field of statistics has been that everyone wants to be a theorist. Part of this is envy - the real sciences are based on mathematical theory. In the universities for this century, the glamor and prestige has been in mathematical models and theorems, no matter how irrelevant. — Leo Breiman (1995) "],["intro.html", "Chapter 1 Introduction 1.1 Nail Finders, The Edifice Complex and Oz 1.2 Where’s the randomness? 1.3 What next?", " Chapter 1 Introduction We start with a certain problem, and I find myself drawing pictures of the problem before we ever get abstract. You’re trying to make the problem concrete before you translate it into the abstract. — in A Conversation with Leo Breiman [1] The inspiration for this manual comes from lecture notes in applied statistics from a course taught at the UC, Berkeley statistics department in 1991, and from the author of these notes, Professor Leo Breiman. In 2001, in an article titled Statistical Modeling: The Two Cultures [2], Leo Breiman called out the statistical community for being ill-equipped to tackle the data analytical challenges of the day. Some 14 years later the field of statistics appeared to finally be shaken out of its lull as many were asking where the field of statistics was going, or not going rather. In 50 Years of Data Science [3], Breiman’s urging academic statistics to expand its boundaries beyond the classical domain of theoretical statistics is recognized along with similar callings coming from contemporaries John Chambers [4], Jeff Wu (slides here), and Bill Cleveland [5] as well as from their predecessor, John Tukey [6]1 , some 50 years before them. For the period running from 2001 to 2014, Semantic Scholar recorded 639 citations to the Two Cultures paper. Since 2014, Semantic Scholar recorded 2603 citations. References in the web are countless. Almost all of the discussion of the paper has to do with the main point of the paper - the predictive versus the data modeling approach to data analysis. In addition to calling out the divide between the two cultures in terms of suitability for tackling modern day problems, in his 2001 manifesto Leo Breiman also points out that even when the data modeling approach is appropriate, statisticians are ill-prepared to solve these problems because of the training by technique taught in schools and consequent misuses of these in practice leading practitioners to view the role of the statistician as one of finding the most appropriate pre-defined technique to solve a problem, rather than trying to solve the problem directly2. The significance of this criticism of common statistical practice has been ignored by most3. While teaching the class the theoretical foundations of applied statistics, Professor Breiman often reminded us to focus on the problem; not the technique, and to clearly identify the sources of variability rather than accept that the variability assumed by any standard model is adequate. He was urging the students to think; not follow recipes. This is central dogma of this monograph: think before you stat. Leo Breiman’s career followed a very interesting and unusual path. He left an academic post as a theoretical probabilist to earn a living as a consultant while teaching mathematics to kids from Mexico. In doing so he became deeply interested in the education system. He ran for and was elected to the Santa Monica School Board and soon after became its president. While he was “having a wonderful time consulting… solving real problems, working with data,” he was offered a faculty position in the Department of Statistics at UC, Berkeley, which he accepted [1,8]4. 1.1 Nail Finders, The Edifice Complex and Oz A short time after returning to academics, Professor Breiman wrote an interesting paper titled “Nail Finders, Edifices, and Oz” [7]. He writes: The thesis of this paper is that many, if not most, statisticians in government and industry are poorly trained for their profession with the result that they are poor problem solvers … While we may disagree as to whether or not statisticians deserve this characterization, we have to agree that the traits described in the paper are best avoided - the find-the-nail, edifice and Oz complex. The find-the-nail complex If all you have is a hammer, then every problem will look like a nail. — Jerome Freidman saying (source unknown) [7] Applied to statisticians, this refers to absorption with the technique (unhealthy infatuation with models?) rather than with the problem; “the failure to see the problem whole; to ask, Does it all make sense?” and, dare I say, “where’s the randomness coming from.” Are statisticians nail-finders? How many of us have encountered survival data at work and Not used the Cox proportional hazards model to analyze the data? How many of us have used something other than logistic or probit regression to analyze data when the outcome is dichotomous. In some cases these techniques are appropriate, but there must be numerous cases when some other approach would be preferable, but never considered. The nail finder syndrome is encouraged to a large extent by the usual textbook format which includes neat examples associated with each introduced technique. This format leads to the belief that there is a ready-made canned solution for every problem. In this modus operandi, little to no effort is spent trying to get to a deep understanding of the problem at hand, and the data to help solve the problem; one only tries to characterize the problem to the extent necessary to find a suitably well matched canned solution. In laying out these notes we will avoid this format. Case studies in data analysis will be laid out in separate sections with no connection to the notes sections, some of which do introduce specific analysis methods. The edifice complex This refers to the building of a large, elaborate and many layered statistical analysis often covering up what is simple and obvious.* Another trap best avoided - inventing complexity when there isn’t any. Not every problem presented to a statistician requires statistical methodologies for their solution. In some cases the solution may reveal itself in a plot or in a sketch which captures the essence of the problem. The Wizard of Oz Complex The exploitation of the mysteries of statistics to dazzle and mystify the less knowledgable. This calls for a little break: Include illustrations of the above - open contributions from XXXers 1.2 Where’s the randomness? In God we trust. All others, bring data and Leave your beliefs at the door5. — adapted from W. Edwards Deming Against this background for the need of change, when we enter Leo Breiman’s classroom we enter a world deeply steeped in the theoretical underpinnings of applied statistics. Although all of the necessary theory was included in the course material, we spent very little time marvelling at the beauty of the mathematical results underlying some statistical theories (for example …) and instead were encouraged and challenged to struggle with difficult questions: what happens if this assumption is not true? or that assumption? I remember one particular lecture watching Professor Breiman repeating the mantra “where is the randomness? where’s the randomness coming from?” while pacing up and down the front of the class. Some 30 years later, faced with the problem of trying to answer questions about a system generating data which comes in the form of batches of points having a yet to be fully understood correlation structure, and subject to sporadic surges of variation of unknown origin, Professor Breiman’s words reverberate in my head. I now fully appreciate the relevance of the question - where’s the randomness coming from. I don’t think there is any exaggeration in claiming that if we can identify the relevant sources of variability, and how to capture them in an analysis dataset, we have basically fully solved the data analytic problem: we know exactly what data to collect and how to summarize them in order to answer the question of interest. 1.3 What next? If the doors of perception were cleansed every thing would appear to man as it is, Infinite — William Blake, The Marriage of Heaven and Hell (1790) The theory The plan is to go through professor Breiman’s notes and to connect the principles and questions raised to problems we face, thinking about the data and sources of variability - where is the randomness coming from. Having gone through these notes, we should be convinced of the truth of this idiom, and this in turn will provide the motivation and drive necessary to get to a full and accurate knowledge of the randomness present in the task at hand: there should never be any guessing or nonchalant presuming involved - the case for any assumed randomness in a data model must be made explicitly6. With this practice we can gain an appropriate level of confidence in the results that we gather to be true, and we should only have to deal with risks coming from well understood variability. Unexpected surprises will become the exception rather than the rule. This is the dream, and it remains a dream while we follow recipes and deliver ready-made solutions rather than tackle problems with an appropriate measure of ingenuity and perseverance. Professor Breiman’s class notes provide the statistical theory necessary to move forward with most data analysis projects. This may be a minimalists’ point of view in terms of required statistical theory background; we will see as we work through some problems. The practice Leo Breiman firmly believed that statistics is about solving problems. We would not be doing justice to his legacy if we just went through his notes which describe some helpful theory whose only relevance in Professor Breiman’s mind was to provide tools to solve problems. With that in mind the essential content of this manuscript will not be the notes on theory, but illustration of applied statistics at work in solving problems. The task is therefore to include numerous case studies illustrating good data analysis practice. To be determined is the lay-out - where to insert the case studies. One thing is certain: we have to avoid the usual format of attaching examples to techniques as they are introduced. The usual textbook format which ties trivial, neat examples to technique as each new technique is introduced leads the reader to make unduly strong association between problem and technique, and to the belief that matching problems to textbook technique is the crunch of statistical analysis7. — Stat Student References "],["preliminaries.html", "Chapter 2 Preliminaries 2.1 Exploratory Data Analysis 2.2 Simulated Data Experimentation", " Chapter 2 Preliminaries Before we get into Brieman’s world we should mention some important elements of data analysis which are not part of the class notes: Exploratory Data Analysis and Data Simulation. 2.1 Exploratory Data Analysis According to George E. P. Box (1980)8. No statistical model can safely be assumed adequate. All models are based on a set of assumptions. The conclusions one can draw from the results sometimes depend as much on the veracity of the assumptions as on the data itself. In many cases, of the assumptions cannot verified to be approximately true, the data should not be analyzed at all. This is a crucial topic which is not discusses in Breiman’s notes. It is also rarely treated to any depths in textbooks which introduce students to data modeling. For one thing the topic is vast and deserves its own course. Another reason is that it is very context specific. There are tools which can be generally applicable and fall under the umbrella term Exploratory data analysis. Originally developed by John Tukey [9] and [10], exploratory data analysis refers to a set of tools and techniques which enable us learn the basic features of a dataset and detect anomalies and unexpected features before proceeding to the more formal probabilistic modeling. Many of the original techniques remain in use today. The omni-present boxplot, for example, was first suggested by John Tukey around that time9. EDA is greatly facilitated by readily available software as well as the modern hardware technologies which can quickly render beautiful plots. This is both great and not so great. It is great when it enables practitioners to do more EDA and let their minds wander unencumbered by technical challenges. It is not so great when the software does the thinking for you while crossing the line between EDA and SDA (statistical …), when the software automatically puts confidence intervals on plots for example10. Trevor Hastie, in a talk which is further explored below (Trevor Hastie SLBD Talk (Bristol 2018) and accompanying SLBD Slides), notes that with the advent of big data, data visualization has become challenging again and some may be tempted skip that essential step of the data analysis process. Professor Hastie remarks that the problems one runs into when analyzing data without familiarising oneself with the salient features of the data may be harder to detect but they have not disappeared. We will not fill this chapter with descriptions of EDA summaries and plots. The methods are varied and many (See [10–12]), and their utility intimately dependent on the context. EDA methods will instead be introduced as they come up in the course of the analyses described below. 2.2 Simulated Data Experimentation when you simulate fake data, you kinda have to have some sense of what’s going on. You’re starting from a position of understanding. — Andrew Gelman Simulations to evaluate assumptions made about the data structure, and for verifying statistical procedures and properties of model fits is another essential part of the data analysis process which didn’t make it into Professor Breiman’s notes. Simulating data appropriately is a non-trivial exercise. The simulated data must conform to the properties of the real it is meant to meant mimic; not necessarily the properties of the data model assumed by the analysis method. In most applications of software engineering, there is no randomness: to each fixed input a specific output is expected and when there are no abends or errors of any kind and the expected output is obtained, the task is basically complete. In statistical analysis and data simulations, once the code works the task has only begun. Once the simulation code “works,” we need to simulate enough fake data to see if it really works - does it capture all of the salient features of the real data; we will look for ways this can be achieved. In addition to his work on data simulations, Andrew Gelman has also published on the topic of multilevel data which may be analogous to the data structure of plated samples coming off the pipeline. See Gilman and Hill (2006) [12], which also has a chapter on simulated data [13] and software to boot (See Data Analysis Using Regression and Multilevel/Hierarchical Model). Other works of interest are Gelman, Hill and Vehtari (2020) [14] and not yet published “Advanced Regression and Multilevel Models.” References "],["what-is-statistics.html", "Chapter 3 What is statistics? 3.1 Probability - inverse statistics 3.2 Statistics - inverse probability", " Chapter 3 What is statistics? Without data, you’re just another person with an opinion. — W. Edwards Deming 3.1 Probability - inverse statistics The worst thing in statistics had to be the p-value. Then came the q-value. — Unknown Origin When statistics first emerged it was known as inverse probability. Michael I. Jordan traces this derivation back the Laplace in the 18th century: Before going on to define statistics, it might be helpful to think about what it is not - probability. (( * Overview of probability or survey of the problems probability deals wuth. Perhaps just enough probability to discuss p-values. )) 3.1.1 p-values The p-value may deserve the lot of being the worst concept in statiistics, but only because it is so rarely used properly. p-values are probabilities and keeping that notion clear is critically important if we want to avoid the many pitfalls associated with p-values, of which there are many. Include a scroll of references documenting problems which arise when p-values are used uncritically, the association with publication requirements and the effect this has had on science. If we keep in mind what p-values are, their mis-use can almost be completely eradicated, at least locally. A p-value is a probability which comes from a probability distribution. What is needed for a p-value to have meaning: replication or repetition with an element of randomness. an understanding of the underlying mechanism giving rise to the randomness - a model for the probability distribution or probability law which accurately captures the randomness in the process underlying the outcome(s) on which the p-values are based. Before quoting a p-value, ask yourself: what is the event? where is the randomness coming from? what is the data generation model and the ensuing probability model assumed by the p-value inference? Or ask yoursel just one question: can I make a bet in favor of, or against the outcome which the p-value refers to? How would the outomce of this wager be resolved? If the answer to these questions don’t easily roll off your tongue, it is probably best not to quote a p-value and stick to descriptive statistics: mean, sd, standard units. Situations when quoting p-values is clearly inappropriate include: One-offs which will never or cannot be replicated. a summary statistic refers to a population has no associated p-value The specifics of how the data were collected in unknown. Statistics encountered during the course of exploratory data analysis. When examining features of study groups which were populated by random assignment. participants from a known population are assigned at random to two groups; what test is implied by a p-value attached to the difference in the proportion of females beteen the two groups? The misuses of p-values can be greatly reduced if we only quote them in the context of an analysis which follows a statistical analysis plan in which a set of hypotheses are clearly described. 3.2 Statistics - inverse probability 3.2.1 The data modeling approach George Box famously said [15]: all models are wrong, but some are useful A far better quotation from the same paper by Box is, Since all models are wrong the scientist must be alert to what is importantly wrong. It is inappropriate to be concerned about mice when there are tigers around11 — Phil Stark (2022) [16] to pull a rabbit from a hat, a rabbit must first be placed in the hat — David A. Freedman The predictive approach the proof of the pudding is in the eating — Proverb dating back to the 14th century wkp What Professor Breiman referred to as the algorithmic approach was later called `the predictive modeling approach by David Donoho [3]. We will adopt that terminology as well. … describe predictive modeling … The predictive data modeling approach High Dimensional Data Some modern data problems which involve the analysis of large datasets can only be solved by a combination of both data and predictive modeling approaches. Omics - genomics, transcriptomics, proteomics, metabolomics and other omics - refer to high-throughput experimental technologies which produce measurements related to the molecular content of cells. Genomic platforms, for example, record measurements indicating the abundance of RNA transcripts present in the cells of processed samples. Different platforms use different technologies to obtain the measurements on transcript abundance, and all platforms require sophisticated pipelines to process the platform’s hardware readout and produce the transcript abundance measurements required for downstream analysis. These in turn require further processing in order to provide data in a form that can be used to address the biological questions of interest, which genes are differentially expressed between two groups of samples for example. First we discuss data analyses which are carried out at the so-called down-stream end of the analysis pipeline, following earlier analyses, sometimes called pre-processing, which transform the machine read-outs into analysis units - vectors of gene expression and protein concentrations, for example. Hastie and colleagues coined the term Statistical Learning to refer to12: a branch of applied statistics that emerged in response to machine learning, emphasizing statistical models and assessment of uncertainty. The primary contrast with Machine Learning, which is a field focused on constructing algorithms that can learn from data, is the incorporation of assessment of uncertainty in the evaluation of a predictor’s performance13. The statistical learning approach is prescribed and illustrated in [17–20]. Examples: microarray data 3.2.1.1 Applied Statistics at Work Another situation which calls for a hybrid between the data modeling approach and the predictive modeling approach to statistics is the work place where data are collected and analyzed for the sole purpose of making decisions and taking actions. In this context it could be argued that even when data modeling is a useful and sometimes necessary port of the analysis, the value of the modeling must be assessed in view of some predictions predicated on the analysis results. Interpretation of fitted models is secondary to their value in making useful predictions. References "],["doe.html", "Chapter 4 Design of Experiments", " Chapter 4 Design of Experiments Statistical procedure and experimental design are only two different aspects of the same whole, and that whole is the logical requirements of the complete process of adding to natural knowledge by experimentation. — Ronald Fisher That’s a mouthful but it makes an important point which is often missed - in order to derive maximum benefit from data, the experimental process must be treated as a whole. When the processes of framing the question, selecting the data to be generated and specifying the analyses to be conducted are not fully integrated a situation is created in which inefficiencies and errors are bound to flourish. The steps which must be taken to produce a useful experimental design are simple to articulate, but difficult to actualize: Start with a full, complete and accurate understanding of the question. Identify the kind of data needed to answer the question. For statistical experiments involving data which come with an element of randomness, when repeated measurements are not guaranteed to produce identical results, a deep understanding of sources of variability is required in order to identify the appropriate sources of data. Specify how the data are to be generated and analyzed. How the data are to be generated, or collected, and analyzed are so intimately related that the two processes should be considered one step. In this day and age of incredible data savvy, or so we all want to believe, it is surprising to see how often statisticians are presented with a data collection proposal and asked to produce N. Even more surprising to me is how often statisticians are able to produce an answered almost immediately. Once the proper data have been identified, and the appropriate way to collect them have been specified, the question of how many samples are needed should already have been answer or would typically be a trivial parameter to pin down in relation to the immense burden required to properly specify all of the antecedental parameters. Many good books provide advice on 3. how to generate and analyze data from experiments. In most, if not all, textbooks, names are given to sets of data and analyses which more or less have common structure - factorial, randomized blocks, incomplete Greco-Latin squares, etc. and we are left to believe that the crunch of the problem is to find the name of the method which fits our data. That’s not problem. The crunch of the problem is figuring out 1 and 2 and no book can properly address the challenges as these will be intimately tied to the specifics of our context. So we will proceed sans le textbook and try to answer these questions by gaining an intimate familiarity with the question at hand, a la Leo Breiman. Will lay out broad principals here - bias, variance, independence - and include Leo Breiman’s Stat 215A thoughts on DOE. "],["linear-models.html", "Chapter 5 Linear Models 5.1 Regression 5.2 Analysis of Variance", " Chapter 5 Linear Models 5.1 Regression 5.2 Analysis of Variance "],["glm.html", "Chapter 6 Generalized Linear Models 6.1 Binary Data and Logictic Models 6.2 Count Data and Log-Linear Models", " Chapter 6 Generalized Linear Models 6.1 Binary Data and Logictic Models 6.2 Count Data and Log-Linear Models "],["predictive-analysis.html", "Chapter 7 Predictive Analysis 7.1 Traditional Classification Analysis 7.2 Modern Predictive Modeling", " Chapter 7 Predictive Analysis 7.1 Traditional Classification Analysis 7.2 Modern Predictive Modeling 7.2.1 CART - The Birth of a Methodology Faced with a new question and data of an unsual format, professor Breiman went on to develop a new methodology which would eventually give rise to … "],["appendix.html", "Chapter 8 Appendix 8.1 Breiman versus Cox 8.2 Freedman on John Snow, Shoe Leather and Lines of Evidence", " Chapter 8 Appendix 8.1 Breiman versus Cox In Statistical Modeling: The Two Cultures [2] Professor Leo Breiman bemoans the fact that a vast majority of statisticians have pigeon-holed themselves to such an extent that the field is becoming irrelevant with the emergence of new data and problems which do not conform to the requirements of classical statistical methodologies. When Professor Breiman returned to academia in 1980 after 13 years of statistical consulting, he was deeply troubled by the prevalent modus operandi in the field of statistics: every article in the Annals of Statistics, the flagship journal of theoretical statistics, started with Assume that the data are generated by the following model, followed by mathematics exploring inference, hypothesis testing and asymptotics. Professor Breiman goes on to describe some of the deficiencies in the applications of statistics which this approach has led to before going on to describe an alternative to the classical statistical data modeling approach, algorithmic modeling14. There are two parts to Professor Breiman’s criticism. One part of the criticism is that some modern day problems are just not amenable to the classical statistical approaches and require a shift in the conventional statistical data analysis paradigm. The other part of the criticism is that even when data modeling is an appropriate way to solve a problem, the traditional approach has led to questionable analyses, results, and conclusions due to uncritical use of standard methods and lack of effort to customize analysis to the specific needs of each problem. One of the statisticians invited to comment on Professor Breiman’s criticism of the current state of affairs in the field of statistics was Sir David Cox, known for, among many other influential works, the Cox proportional hazards model. We will focus on his comments as his disagreement with the notions put forth professor Breiman are the strongest among the commentators. “Professor Breiman takes data as his starting point. I would prefer to start with an issue, a question or a scientific hypothesis” Nobody would seriously argue that starting with anything other than the question is a good idea, but the tendency of practitioners to assume convenient data models, and use the canned, turn-key analytical procedures which come with these models, suggests that these practitioners are quite content with starting with the solution, which one might argue is an even bigger assault to common sense than starting with the data. “Professor Breiman emphasizes prediction as the objective, success at prediction being the criterion of success, as contrasted with issues of interpretation or understanding.” This is another moot point. When we can confidently proceed with a defendable data model, the assessment of the utility of predictions made with the model can only enhance interpretation. “Often the prediction is under quite different conditions from the data. ie. it may be desired to predict the consequences of something only indirectly addressed by the data available for analysis.” Making predictions to a space beyond the space spanned by the data used to estimate the model would be hazardous, no matter what modeling approach is used. After describing an idyllic data analysis package and explaining why the details of such are not published, Sir David goes on to say “By contrast, Professor Breiman equates mainstream applied statistics to a relatively mechanical process involving somehow or other choosing a model, often a default model of standard form, and applying standard methods of analysis and goodness-of-fit procedures. … It is true that many of the analyses done by non-statisticians or by statisticians under severe time constraints are more or less like those Professor Breiman describes.” Professor Cox admits that this automated or mechanical process is likely to be favored when working under severe time constraints; What Sir David Cox doesn’t realize is that this is part of the job description for a statistician working in industry - must be prepared to work under severe time constraints. I am a witness to the fact that the relatively mechanical process described by Professor Breiman is mainstream applied statistics in my industry15. Professor Cox goes on to forgive the laps in rigor in the application of statistical techniques to analyse data with the observation that “one suspects that quite often the limitations of conclusions lie more in weakness of data quality and study design than in ineffective analysis.” How would a practitioner who uncritically uses the common modern day approach ever become aware of the weakness of the data or the design? The answer is likely to be never, as careful consideration is rarely given to the suitability of the model or the quality of the data. A system in which there are no checks and balances in place to ensure the lasting validity of results, but only uncritically rewards adherance to uninfomed timelines, provides little incentive for data analysis professionals to incorporate careful scrutiny of assumptions or innovative solutions in their practice. — Stat Student 8.2 Freedman on John Snow, Shoe Leather and Lines of Evidence Notes [21]: Snow’s work on cholera is presented as a success story for scientific reasoning based on nonexperimental data. Failure stories are also discussed, and comparisons may provide some insight. In particular, this paper suggests that statistical technique can seldom be an adequate substitute for good design, relevant data, and testing predictions against reality in a variety of settings. [22]: Proportional-hazards models are frequently used to analyze data from randomized controlled trials. This is a mistake. Randomization does not justify the models, which are rarely informative. Simpler analytic methods should be used first. Also see [28]. References "],["references.html", "References", " References "]]
