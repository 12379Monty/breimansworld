#  <span style="color:blue">Appendix</span>
##  <span style="color:blue">Breiman versus Cox </span> 

 

In Statistical Modeling: The Two Cultures [@Breiman:2001aa] 
Professor Leo Breiman bemoans the fact that a vast majority 
of statisticians have pigeon-holed themselves to such an extent 
that the field is becoming irrelevant with the emergence of new data
and problems which do not conform to the requirements of classical 
statistical methodologies.  When Professor Breiman returned to academia in
1980 after 13 years of statistical consulting, he was deeply troubled 
by the prevalent modus operandi in the field of statistics:
*every article  in the Annals of Statistics,
the flagship journal of theoretical statistics, started with*
**Assume that the data are generated by the following model**,
followed by mathematics exploring inference, hypothesis testing and
asymptotics.  Professor Breiman goes on to describe some of
the deficiencies in the applications of statistics which this
approach has led to before going on to describe an alternative to
the classical statistical data modeling approach, `algorithmic modeling`^[
Donoho [@Donoho:2017aa] pefers the term `predictive modeling`]. 

**There are two parts to Professor Breiman's criticism**. 
One part of the criticism is that some modern day problems
are just not amenable to the classical statistical approaches and
require a shift in the conventional statistical data analysis paradigm.
The other part of the criticism is that even when data modeling is an
appropriate way to solve a problem, the traditional approach has
led to questionable analyses, results, and conclusions due
to uncritical use of standard methods and lack of effort
to customize analysis to the specific needs of each problem.

One of the statisticians invited to comment on Professor Breiman's
criticism of the current state of affairs in the field of statistics was
Sir David Cox, known for, among many other influential works,
the **Cox proportional hazards model**.  We will focus on his comments as
his disagreement with the notions put forth professor Breiman are
the strongest among the commentators.

<p><p/>
* "Professor Breiman takes data as his starting point.
I would prefer to start with an issue, a question or a scientific hypothesis"
   - Nobody would seriously argue that  starting with anything other than
the question is a good idea, but the tendency of practitioners to
assume convenient data models, and use the canned, turn-key analytical procedures
which come with these models, suggests that these practitioners are
quite content with starting with the solution, which one might
argue is an even bigger assault to common sense than starting with 
the data.

<p><p/>
* "Professor Breiman emphasizes prediction as the objective,
success at prediction being the criterion of success,
as contrasted with issues of interpretation or understanding".
   - This is another moot point. When we can confidently proceed with
a defendable data model, the assessment of the utility of predictions
made with the model can only enhance interpretation.

<p><p/>
* "Often the prediction is under quite different conditions from the data.
ie. it may be desired to predict the consequences of something only
indirectly addressed by the data available for  analysis."
   - Making predictions to a space beyond the space spanned by
the data used to estimate the model would be hazardous,
no matter what  modeling approach is used.

<p><p/>
* After describing an idyllic data analysis workflow and explaining why 
the details of such are rarely published, Sir David goes on to assert 
"By contrast, Professor Breiman equates mainstream applied statistics
to a relatively mechanical process involving somehow or other
choosing a model, often a default model of standard form, and
applying standard methods of analysis and goodness-of-fit procedures.
... It is true that many of the analyses done by non-statisticians or
by statisticians under severe time constraints are more or less like
those Professor Breiman describes."
   - Professor Cox admits that this automated or mechanical
process is likely to  be favored when working under severe time constraints;  
      - What Sir David Cox doesn't realize is that this is part of the job description
for a statistician working in industry - must be prepared to work
under severe time constraints.  I am a witness to the fact that the 
relatively mechanical process described by Professor Breiman **is** 
mainstream applied statistics in my industry^[
I am not prepared to put the blame entirely on the
lack of preparation statistics graduates might have
as time on the job does not seem to translate to
improved behaviours - more careful examination of the
questions and the development of methods suited to 
the particulars of the problems.  Some blame has
to be attributed to the system in place.].

<p><p/>
* Professor Cox goes on to forgive the laps in rigor in the application of
statistical techniques to analyse data with the observation that
"one suspects that quite often the limitations of conclusions lie 
more in weakness of data quality and study design than in ineffective analysis."
How would a practitioner who uncritically uses the common modern day 
approach ever become aware of the weakness of the data or the design?
The answer is likely to be never, as careful consideration is rarely 
given to the suitability of the model or the quality of the data.


<span style="color:blue">

> **A system in which there are no checks and balances 
> in place to ensure the lasting validity of results,
> but only uncritically rewards adherance to uninformed
> timelines, provides little incentive for data analysis
> professionals to incorporate careful scrutiny of assumptions 
> or innovative solutions in their practice**.
>`r tufte::quote_footer('--- Stat Student')`

</span> 

<p><p/>

## Freedman on John Snow, Shoe Leather and Lines of Evidence 


Notes  

* [@Freedman:1991aa]:
   - Snowâ€™s work on cholera is presented as a success story for 
scientific reasoning based on nonexperimental data. Failure stories are 
also discussed, and comparisons may provide some insight. In particular, 
this paper suggests that statistical technique can seldom be an adequate 
substitute for good design, relevant data, and testing predictions against reality 
in a variety of settings.

* [@Freedman:2008aa]: 
   - Proportional-hazards models are frequently used to analyze data from 
randomized controlled trials. This is a mistake. Randomization does not 
justify the models, which are rarely informative. Simpler analytic methods 
should be used first.

Also see [@Freedman:2009aa,@Freedman:1999aa,@Freedman:2004aa,@Freedman:2009ab,@Freedman:2008aa,@Freedman:1991aa,@Freedman:2009ac,@Freedman:2008ab].


